Анализатор (паук) веб-сайтов для сбора необходимой информации и передачи на сервер проекта.

### Общее описание
Паук оформлен в виде микросервиса с одним конфигурационным файлом. Построен с
использованием библиотеки "dropwizard.io" и сопутствующих библиотек инфраструктуры.

### Сборка
Сборка итогового компонента проекта осуществляется с помощью gradle plugin'а
"shadowJar" (com.github.johnrengelman.shadow), зависящего от задачи "build". Так
что для создания итогового файла необходимо запускать "gradle shadowJar"

### Запуск и отладка итогового микросервиса
Для запуска в рабочем и отладочноме режиме используются скриптовые файлы в корне проекта
"run.sh" и "run_debug.sh" соответственно.

### Конфигурационный файл
Далее описана структура конфигурационного файла
```
---
# Сохранять файлы или производить запись в БД?  (только для отладки)
store_files: false
# Количество потоков на сайт (лучше не более 4)
crawler_per_site: 1
# Папка со скриптами для парсинга контента сайтов
script_dir: /tmp/crawler/scripts
# Блок с настройками для анализируемых сайтов
sites:
   # На каждый сайт. Название домена (будет использоваться для идентфикации и записи в БД)
 - domain: bnkomi.ru
   # На каждый сайт. стартовая страница для парсинга
   seed: http://bnkomi.ru
# Каталог для сохранения данных о результатах парсинга сайта (для каждого сайта
# создается подпапка с именем домена для хранения соотвествующих данных)
storage_dir: /tmp/crawler
# строка подключения к MongoDB для сохранения результатаов анализа
connection_url: mongodb://localhost:27017/
```
### Журналирование

### Метрики

### Запись в БД
Уникальность записи в БД определяется на основании пары - (domain:URL)
